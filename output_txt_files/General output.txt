## STARTING DOCKER and running Prediction ## ** START **
Step 5/6 : EXPOSE 80
 ---> Running in 90a3e611e1fc
Removing intermediate container 90a3e611e1fc
 ---> d8e1c713480e
Step 6/6 : CMD ["python", "app.py"]
 ---> Running in 0038b413e0be
   Use a production WSGI server instead.
 * Debug mode: on
 * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 194-414-639
[2022-07-04 08:49:10,315] INFO in app: JSON payload:
{'CHAS': {'0': 0}, 'RM': {'0': 6.575}, 'TAX': {'0': 296.0}, 'PTRATIO': {'0': 15.3}, 'B': {'0': 396.9}, 'LSTAT': {'0': 4.98}}

[2022-07-04 08:49:10,425] INFO in app: Inference payload DataFrame:
   CHAS     RM    TAX  PTRATIO      B  LSTAT
0     0  6.575  296.0     15.3  396.9   4.98
[2022-07-04 08:49:10,436] INFO in app: Scaling Payload:
   CHAS     RM    TAX  PTRATIO      B  LSTAT
0     0  6.575  296.0     15.3  396.9   4.98
172.17.0.1 - - [04/Jul/2022 08:49:10] "POST /predict HTTP/1.1" 200 -
## STARTING DOCKER and running Prediction ## ** END **

## Prediction ## ** START **
sunil@SunilPersonalLaptop:/mnt/d/Personal/Learnings/Cloud/Udacity Nano Degree for Cloud DevOps/Project-4/project-ml-microservice-kubernetes$ ./make_prediction.sh
Port: 8000
{
  "prediction": [
    20.35373177134412
  ]
}

## Prediction ## ** END **